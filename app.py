import streamlit as st
import os
import tempfile
import mne
import pandas as pd
from nilearn import plotting, image
import google.generativeai as genai

# CONFIGURA√á√ÉO DE SEGURAN√áA: Puxando a chave dos Secrets
try:
    GENAI_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GENAI_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash') # Vers√£o r√°pida e inteligente
except Exception as e:
    st.error("Erro na Chave API: Verifique se configurou o 'Secrets' no Streamlit.")

st.set_page_config(page_title="NeuroLab Gemini", page_icon="üß†", layout="wide")

st.title("üß† NeuroLab + Gemini AI 1.5")
st.caption("Assistente de Neuroengenharia conectado ao c√©rebro do Google.")

# Mem√≥ria do Chat
if "mensagens" not in st.session_state:
    st.session_state.mensagens = [{"role": "assistant", "content": "Ol√°, Sabrina! O Gemini est√° online. Fa√ßa o upload do arquivo para come√ßarmos a an√°lise real."}]

# ============================================
# BARRA LATERAL
# ============================================
with st.sidebar:
    st.header("üìÇ Entrada")
    arquivo_carregado = st.file_uploader("Upload BIDS:", type=["edf", "set", "vhdr", "nii", "nii.gz", "tsv", "csv"])

# ============================================
# LAYOUT E PROCESSAMENTO
# ============================================
col_visual, col_chat = st.columns([1.2, 1])
dados_objeto = None
contexto_tecnico = "Nenhum dado carregado."

with col_visual:
    st.subheader("üìä Visualizador")
    if arquivo_carregado:
        ext = os.path.splitext(arquivo_carregado.name)[1].lower()
        if arquivo_carregado.name.endswith(".nii.gz"): ext = ".nii.gz"
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
            tmp.write(arquivo_carregado.getvalue())
            path = tmp.name

        try:
            if ext in ['.nii', '.nii.gz']:
                dados_objeto = image.index_img(path, 0)
                st.components.v1.html(plotting.view_img(dados_objeto, bg_img=False).get_iframe(), height=450)
                contexto_tecnico = f"Arquivo MRI NIfTI. Dimens√µes: {dados_objeto.shape}. "
            
            elif ext in ['.edf', '.set', '.vhdr']:
                try:
                    dados_objeto = mne.io.read_raw(path, preload=True, verbose=False)
                except:
                    dados_objeto = mne.io.read_epochs_eeglab(path, verbose=False)
                
                st.pyplot(dados_objeto.plot(n_channels=10, show=False, scalings='auto'))
                contexto_tecnico = f"EEG com {len(dados_objeto.ch_names)} canais: {dados_objeto.ch_names}. Freq: {dados_objeto.info['sfreq']}Hz."
        except Exception as e:
            st.error(f"Erro: {e}")

# ============================================
# CHAT REAL COM GEMINI
# ============================================
with col_chat:
    st.subheader("Assistente Gemini ‚ú®")
    
    chat_box = st.container(height=450)
    for m in st.session_state.mensagens:
        chat_box.chat_message(m["role"]).markdown(m["content"])

    if prompt := st.chat_input("Pergunte qualquer coisa sobre o arquivo..."):
        st.session_state.mensagens.append({"role": "user", "content": prompt})
        chat_box.chat_message("user").markdown(prompt)

        with chat_box.chat_message("assistant"):
            with st.spinner("Gemini pensando..."):
                # CRIANDO O PROMPT "ESPECIALIZADO" PARA O GEMINI
                instrucao_ia = f"""
                Voc√™ √© uma IA especialista em Neuroengenharia e pesquisadora da UFABC. 
                Contexto do arquivo atual: {contexto_tecnico}.
                Pergunta da Sabrina: {prompt}
                Responda de forma t√©cnica, por√©m objetiva, citando conceitos cient√≠ficos se necess√°rio.
                """
                
                try:
                    response = model.generate_content(instrucao_ia)
                    texto_resposta = response.text
                except Exception as e:
                    texto_resposta = f"Ops, tive um erro ao falar com o Gemini: {e}"

                st.markdown(texto_resposta)
                st.session_state.mensagens.append({"role": "assistant", "content": texto_resposta})
        st.rerun()
